{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"..\", \".env\"), override=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3ee5b",
   "metadata": {},
   "source": [
    "# Deep Agent for Research\n",
    "\n",
    "## Overview \n",
    "\n",
    "<img src=\"./assets/agent_header.png\" width=\"800\" style=\"display:block; margin-left:0;\">\n",
    "\n",
    "Now, we can put everything we have learned together:\n",
    "\n",
    "* We will use **TODOs** to keep track of tasks. \n",
    "* We will use **files** to store raw tool call results. \n",
    "* We will **delegate research tasks to sub-agents** for context isolation. \n",
    "\n",
    "## Search Tool \n",
    "\n",
    "We'll build a search tool that offloads raw contents to files and returns only a summary to the agent. This is a common pattern for long-running agent trajectories, [as we've seen with Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)! \n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Search Execution (`run_tavily_search`)**: Performs the actual web search using Tavily API with configurable parameters for results count and topic filtering.\n",
    "\n",
    "2. **Content Summarization (`summarize_webpage_content`)**: Uses a lightweight model (GPT-4o-mini) to generate structured summaries of webpage content, producing both a descriptive filename and key learnings summary.\n",
    "\n",
    "3. **Result Processing (`process_search_results`)**: Fetches full webpage content via HTTP, converts HTML to markdown using `markdownify`, and generates summaries for each result.\n",
    "\n",
    "4. **Context Offloading (`tavily_search` tool)**: The main tool that:\n",
    "   - Executes search and processes results\n",
    "   - Saves full content to files in agent state (context offloading)\n",
    "   - Returns only minimal summaries to the agent (prevents context spam)\n",
    "   - Uses LangGraph `Command` to update both files and messages\n",
    "\n",
    "5. **Strategic Thinking (`think_tool`)**: Provides a structured reflection mechanism for agents to analyze findings, assess gaps, and plan next steps in their research workflow.\n",
    "\n",
    "This architecture solves the token efficiency problem by storing detailed search results in files while keeping the agent's working context minimal and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bae934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/deep_agents_from_scratch/research_tools.py\n",
    "\"\"\"Research Tools.\n",
    "\n",
    "This module provides search and content processing utilities for the research agent,\n",
    "including web search capabilities and content summarization tools.\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid, base64\n",
    "\n",
    "import httpx\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.tools import InjectedToolArg, InjectedToolCallId, tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "from markdownify import markdownify\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "from typing_extensions import Annotated, Literal\n",
    "\n",
    "from deep_agents_from_scratch.prompts import SUMMARIZE_WEB_SEARCH\n",
    "from deep_agents_from_scratch.state import DeepAgentState\n",
    "\n",
    "# Summarization model \n",
    "summarization_model = init_chat_model(model=\"gemini-2.5-flash-exp\", model_provider=\"google-genai\", max_tokens=1000)\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    filename: str = Field(description=\"Name of the file to store.\")\n",
    "    summary: str = Field(description=\"Key learnings from the webpage.\")\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %-d, %Y\")\n",
    "\n",
    "def run_tavily_search(\n",
    "    search_query: str, \n",
    "    max_results: int = 1, \n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", \n",
    "    include_raw_content: bool = True, \n",
    ") -> dict:\n",
    "    \"\"\"Perform search using Tavily API for a single query.\n",
    "\n",
    "    Args:\n",
    "        search_query: Search query to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        Search results dictionary\n",
    "    \"\"\"\n",
    "    result = tavily_client.search(\n",
    "        search_query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def summarize_webpage_content(webpage_content: str) -> Summary:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "    \n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "        \n",
    "    Returns:\n",
    "        Summary object with filename and summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up structured output model for summarization\n",
    "        structured_model = summarization_model.with_structured_output(Summary)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_and_filename = structured_model.invoke([\n",
    "            HumanMessage(content=SUMMARIZE_WEB_SEARCH.format(\n",
    "                webpage_content=webpage_content, \n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        return summary_and_filename\n",
    "        \n",
    "    except Exception:\n",
    "        # Return a basic summary object on failure\n",
    "        return Summary(\n",
    "            filename=\"search_result.md\",\n",
    "            summary=webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "        )\n",
    "\n",
    "def process_search_results(results: dict) -> list[dict]:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "    \n",
    "    Args:\n",
    "        results: Tavily search results dictionary\n",
    "        \n",
    "    Returns:\n",
    "        List of processed results with summaries\n",
    "    \"\"\"\n",
    "    processed_results = []\n",
    "\n",
    "    # Create a client for HTTP requests\n",
    "    HTTPX_CLIENT = httpx.Client()\n",
    "    \n",
    "    for result in results.get('results', []):\n",
    "        \n",
    "        # Get url \n",
    "        url = result['url']\n",
    "        \n",
    "        # Read url\n",
    "        response = HTTPX_CLIENT.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Convert HTML to markdown\n",
    "            raw_content = markdownify(response.text)\n",
    "            summary_obj = summarize_webpage_content(raw_content)\n",
    "        else:\n",
    "            # Use Tavily's generated summary\n",
    "            raw_content = result.get('raw_content', '')\n",
    "            summary_obj = Summary(\n",
    "                filename=\"URL_error.md\",\n",
    "                summary=result.get('content', 'Error reading URL; try another search.')\n",
    "            )\n",
    "        \n",
    "        # uniquify file names\n",
    "        uid = base64.urlsafe_b64encode(uuid.uuid4().bytes).rstrip(b\"=\").decode(\"ascii\")[:8]\n",
    "        name, ext = os.path.splitext(summary_obj.filename)\n",
    "        summary_obj.filename = f\"{name}_{uid}{ext}\"\n",
    "\n",
    "        processed_results.append({\n",
    "            'url': result['url'],\n",
    "            'title': result['title'],\n",
    "            'summary': summary_obj.summary,\n",
    "            'filename': summary_obj.filename,\n",
    "            'raw_content': raw_content,\n",
    "        })\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    max_results: Annotated[int, InjectedToolArg] = 1,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> Command:\n",
    "    \"\"\"Search web and save detailed results to files while returning minimal context.\n",
    "\n",
    "    Performs web search and saves full content to files for context offloading.\n",
    "    Returns only essential information to help the agent decide on next steps.\n",
    "\n",
    "    Args:\n",
    "        query: Search query to execute\n",
    "        state: Injected agent state for file storage\n",
    "        tool_call_id: Injected tool call identifier\n",
    "        max_results: Maximum number of results to return (default: 1)\n",
    "        topic: Topic filter - 'general', 'news', or 'finance' (default: 'general')\n",
    "\n",
    "    Returns:\n",
    "        Command that saves full results to files and provides minimal summary\n",
    "    \"\"\"\n",
    "    # Execute search\n",
    "    search_results = run_tavily_search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    ) \n",
    "\n",
    "    # Process and summarize results\n",
    "    processed_results = process_search_results(search_results)\n",
    "    \n",
    "    # Save each result to a file and prepare summary\n",
    "    files = state.get(\"files\", {})\n",
    "    saved_files = []\n",
    "    summaries = []\n",
    "    \n",
    "    for i, result in enumerate(processed_results):\n",
    "        # Use the AI-generated filename from summarization\n",
    "        filename = result['filename']\n",
    "        \n",
    "        # Create file content with full details\n",
    "        file_content = f\"\"\"# Search Result: {result['title']}\n",
    "\n",
    "**URL:** {result['url']}\n",
    "**Query:** {query}\n",
    "**Date:** {get_today_str()}\n",
    "\n",
    "## Summary\n",
    "{result['summary']}\n",
    "\n",
    "## Raw Content\n",
    "{result['raw_content'] if result['raw_content'] else 'No raw content available'}\n",
    "\"\"\"\n",
    "        \n",
    "        files[filename] = file_content\n",
    "        saved_files.append(filename)\n",
    "        summaries.append(f\"- {filename}: {result['summary']}...\")\n",
    "    \n",
    "    # Create minimal summary for tool message - focus on what was collected\n",
    "    summary_text = f\"\"\"ðŸ” Found {len(processed_results)} result(s) for '{query}':\n",
    "\n",
    "{chr(10).join(summaries)}\n",
    "\n",
    "Files: {', '.join(saved_files)}\n",
    "ðŸ’¡ Use read_file() to access full details when needed.\"\"\"\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"files\": files,\n",
    "            \"messages\": [\n",
    "                ToolMessage(summary_text, tool_call_id=tool_call_id)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "\n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "\n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    - How complex is the question: Have I reached the number of search limits?\n",
    "\n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "\n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "\n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd74759",
   "metadata": {},
   "source": [
    "## Deep Agent\n",
    "\n",
    "Now, we can just apply all of our prior learnings: \n",
    "\n",
    "* We'll give the researcher a `think_tool` and our `search_tool` above.\n",
    "* We'll give our parent agent file tools, a `think_tool`, and a `task` tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import show_prompt, stream_agent\n",
    "\n",
    "from deep_agents_from_scratch.file_tools import ls, read_file, write_file\n",
    "from deep_agents_from_scratch.prompts import (\n",
    "    FILE_USAGE_INSTRUCTIONS,\n",
    "    RESEARCHER_INSTRUCTIONS,\n",
    "    SUBAGENT_USAGE_INSTRUCTIONS,\n",
    "    TODO_USAGE_INSTRUCTIONS,\n",
    ")\n",
    "from deep_agents_from_scratch.research_tools import tavily_search, think_tool, get_today_str\n",
    "from deep_agents_from_scratch.state import DeepAgentState\n",
    "from deep_agents_from_scratch.task_tool import _create_task_tool\n",
    "from deep_agents_from_scratch.todo_tools import write_todos, read_todos\n",
    "\n",
    "# Create agent using create_react_agent directly\n",
    "model = init_chat_model(model=\"gemini-2.5-flash-exp\", model_provider=\"google-genai\", max_tokens=1000)\n",
    "\n",
    "# Limits\n",
    "max_concurrent_research_units = 3\n",
    "max_researcher_iterations = 3\n",
    "\n",
    "# Tools\n",
    "sub_agent_tools = [tavily_search, think_tool]\n",
    "built_in_tools = [ls, read_file, write_file, write_todos, read_todos, think_tool]\n",
    "\n",
    "# Create research sub-agent\n",
    "research_sub_agent = {\n",
    "    \"name\": \"research-agent\",\n",
    "    \"description\": \"Delegate research to the sub-agent researcher. Only give this researcher one topic at a time.\",\n",
    "    \"prompt\": RESEARCHER_INSTRUCTIONS.format(date=get_today_str()),\n",
    "    \"tools\": [\"tavily_search\", \"think_tool\"],\n",
    "}\n",
    "\n",
    "# Create task tool to delegate tasks to sub-agents\n",
    "task_tool = _create_task_tool(\n",
    "    sub_agent_tools, [research_sub_agent], model, DeepAgentState\n",
    ")\n",
    "\n",
    "delegation_tools = [task_tool]\n",
    "all_tools = sub_agent_tools + built_in_tools + delegation_tools  # search available to main agent for trivial cases\n",
    "\n",
    "# Build prompt\n",
    "SUBAGENT_INSTRUCTIONS = SUBAGENT_USAGE_INSTRUCTIONS.format(\n",
    "    max_concurrent_research_units=max_concurrent_research_units,\n",
    "    max_researcher_iterations=max_researcher_iterations,\n",
    "    date=datetime.now().strftime(\"%a %b %-d, %Y\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb7a89-8a26-4fb4-ba77-b05180f2c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prompt(RESEARCHER_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = (\n",
    "    \"# TODO MANAGEMENT\\n\"\n",
    "    + TODO_USAGE_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"=\" * 80\n",
    "    + \"\\n\\n\"\n",
    "    + \"# FILE SYSTEM USAGE\\n\"\n",
    "    + FILE_USAGE_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"=\" * 80\n",
    "    + \"\\n\\n\"\n",
    "    + \"# SUB-AGENT DELEGATION\\n\"\n",
    "    + SUBAGENT_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "show_prompt(INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a13c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "agent = create_react_agent(\n",
    "    model, all_tools, prompt=INSTRUCTIONS, state_schema=DeepAgentState\n",
    ")\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042e655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import format_messages\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me an overview of Model Context Protocol (MCP).\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dd00b",
   "metadata": {},
   "source": [
    "Trace: \n",
    "https://smith.langchain.com/public/3a389ec6-8e6e-4f9e-9a82-0d0a9569e6f8/r\n",
    "<!-- https://smith.langchain.com/public/1df7a10e-1465-499c-a3e0-86c1d5429324/r -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0df5d",
   "metadata": {},
   "source": [
    "## Using the Deep Agent Package\n",
    "\n",
    "Now you understand the underlying patterns! \n",
    "\n",
    "You can [use the `deepagents` package](\n",
    "https://github.com/hwchase17/deepagents) as a simple abstraction:\n",
    "\n",
    "* It include the file system tools\n",
    "* It includes the todo tool\n",
    "* It includes the task tool\n",
    "\n",
    "You only need to supply the sub-agent and any tools you want the sub-agent to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepagents import create_deep_agent\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    sub_agent_tools,\n",
    "    INSTRUCTIONS,\n",
    "    subagents=[research_sub_agent],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613634c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me an very brief overview of Model Context Protocol (MCP).\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc6784",
   "metadata": {},
   "source": [
    "Trace: \n",
    "https://smith.langchain.com/public/1d626d81-a102-4588-a2fb-cab40a7271f1/r\n",
    "<!-- https://smith.langchain.com/public/1ae2d7f6-f901-4ebd-b6c3-6657a55f88ae/r -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73925c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-from-scratch (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
